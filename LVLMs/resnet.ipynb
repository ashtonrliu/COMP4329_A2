{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1770bedb",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a9199458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, csv\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc0efb",
   "metadata": {},
   "source": [
    "# 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "217332eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILENAME = 'train.csv'\n",
    "TEST_FILENAME = 'test.csv'\n",
    "IMAGE_DIRECTORY = \"images\"\n",
    "DATA_BASEPATH = \"../dataset\"\n",
    "LABELS = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '13', '14', '15', '16', '17', '18', '19']\n",
    "NUM_OF_LABELS = len(LABELS)\n",
    "RESIZED_SIZE = 320 # All images are at most 320x320 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dcddb",
   "metadata": {},
   "source": [
    "# 3. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c0d17584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_none_to_caption(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Append non-null entries from the 'None' column into the caption column, then drop the 'None' column.\n",
    "    \"\"\"\n",
    "    caption_col = \"Caption\"\n",
    "    extra_col = None\n",
    "    sep = \" \"\n",
    "\n",
    "    if extra_col is None:\n",
    "        if None in df.columns:\n",
    "            extra_col = None\n",
    "        elif 'None' in df.columns:\n",
    "            extra_col = 'None'\n",
    "        else:\n",
    "            raise KeyError(\"Could not find a column named None or 'None' in your DataFrame.\")\n",
    "\n",
    "    # make sure captions are strings\n",
    "    df[caption_col] = df[caption_col].astype(str)\n",
    "\n",
    "    # build a Series of the extra text, safely capturing sep in the lambda’s default\n",
    "    extras = df[extra_col].apply(\n",
    "        lambda val, sep=sep: (\n",
    "            '' if pd.isna(val)\n",
    "            else sep.join(str(item).strip() for item in (val if isinstance(val, (list, tuple)) else [val]))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # only append where there actually is some extra text\n",
    "    mask = extras.ne('')\n",
    "    df.loc[mask, caption_col] = df.loc[mask, caption_col] + sep + extras[mask]\n",
    "\n",
    "    return df.drop(columns=[extra_col])\n",
    "\n",
    "def extract_df(filename):\n",
    "    path = os.path.join(DATA_BASEPATH, filename)\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
    "        df = pd.DataFrame(reader)\n",
    "\n",
    "    return append_none_to_caption(df)\n",
    "\n",
    "train_data = extract_df(TRAIN_FILENAME)\n",
    "test_data = extract_df(TEST_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01e9bb",
   "metadata": {},
   "source": [
    "# 4. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7869de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_to_index_dictionary():\n",
    "    \"\"\"\n",
    "    A dictionary which transforms labels from [1, 19] to an index used for one-hot encoding\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    index = 0\n",
    "    for label in LABELS:\n",
    "        dictionary[label] = index\n",
    "        index += 1\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "label_to_index_dict = create_label_to_index_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3771177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_resize(img, target_size: int = RESIZED_SIZE, pad_color=(0, 0, 0)) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Resize a PIL image so the longer side equals `target_size`, keeping aspect ratio,\n",
    "    then pad the shorter side with `pad_color` to make a square image of size target_sizextarget_size.\n",
    "    \n",
    "    Args:\n",
    "        img (PIL.Image): input image\n",
    "        target_size (int): final square side length (e.g. 320)\n",
    "        pad_color (tuple/int): RGB or grayscale pad value\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: 320x320 letter-boxed image\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    \n",
    "    # ---- 1. Compute new size that preserves aspect ratio ---- #\n",
    "    if w > h:\n",
    "        new_w = target_size\n",
    "        new_h = int(round(h * target_size / w))\n",
    "    else:\n",
    "        new_h = target_size\n",
    "        new_w = int(round(w * target_size / h))\n",
    "    \n",
    "    # ---- 2. Resize with high-quality interpolation ---- #\n",
    "    img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "    \n",
    "    # ---- 3. Create padded canvas and paste resized image ---- #\n",
    "    pad_w = target_size - new_w\n",
    "    pad_h = target_size - new_h\n",
    "    \n",
    "    # Center the image (optional: randomize offsets for extra augmentation)\n",
    "    left   = pad_w // 2\n",
    "    top    = pad_h // 2\n",
    "\n",
    "    if img.mode == \"RGB\":\n",
    "        pad_color = pad_color if isinstance(pad_color, tuple) else (pad_color,)*3\n",
    "    else:\n",
    "        pad_color = pad_color if isinstance(pad_color, int) else pad_color[0]\n",
    "    \n",
    "    new_img = Image.new(img.mode, (target_size, target_size), pad_color)\n",
    "    new_img.paste(img, (left, top))\n",
    "    \n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "89d46050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecord:\n",
    "\n",
    "    def __init__(self, id, image, caption, labels):\n",
    "        \"\"\"\n",
    "        Collects and stores relevant information about each data entry.\n",
    "        Attributes:\n",
    "            - id, retrieved from the filename\n",
    "            - image, a PIL image best used for displaying\n",
    "            - label, a string containing labels. If there is more than one, separated by a space\n",
    "            - captions, a string containing a description of the image\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        # self.image  = image \n",
    "        self.image = letterbox_resize(image) # Reshapes original image to 320x320\n",
    "        self.caption = caption\n",
    "        self.label  = labels\n",
    "\n",
    "        if labels != None:\n",
    "            self.one_hot_encode = self.create_one_hot_vector() # Transforms label into one_hot_encoding\n",
    "    \n",
    "    def display_data(self):\n",
    "        plt.imshow(self.image)\n",
    "        plt.axis('off')      \n",
    "        plt.title(f\"ImageID: {self.id} Label: {self.label}\\n{self.caption}\", wrap=True)\n",
    "        plt.show()\n",
    "\n",
    "    def get_filename(self):\n",
    "        return f\"{self.id}.jpg\"\n",
    "    \n",
    "    def create_one_hot_vector(self):\n",
    "        labels = self.label.split(\" \")\n",
    "\n",
    "        one_hot_encode = np.zeros(NUM_OF_LABELS)\n",
    "        for label in labels:\n",
    "            one_hot_encode[label_to_index_dict.get(label)] = 1  \n",
    "        \n",
    "        return one_hot_encode\n",
    "\n",
    "def retrieve_image(filename):\n",
    "    \"\"\"\n",
    "    Returns an image from a given filename\n",
    "    \"\"\"\n",
    "    return Image.open(f'{DATA_BASEPATH}/{IMAGE_DIRECTORY}/{filename}')\n",
    "\n",
    "def create_all_samples(dataframe, is_training=True):\n",
    "    \"\"\"\n",
    "    Uses given dataframe to generate all samples which contain an id, image, label and caption.\n",
    "\n",
    "    Returns a list of samples\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    label = None\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        filename = row[\"ImageID\"]\n",
    "        image = retrieve_image(filename)\n",
    "        caption = row[\"Caption\"]\n",
    "\n",
    "        if is_training:\n",
    "\n",
    "            label = row[\"Labels\"]\n",
    "\n",
    "        sample = ImageRecord(filename[:-4], image, caption, label)\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "train_samples = create_all_samples(train_data)\n",
    "# test_samples = create_all_samples(test_data, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb698ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e54f6c5",
   "metadata": {},
   "source": [
    "# 5. Resnet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a390d5",
   "metadata": {},
   "source": [
    "### 5.1 Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "adc1f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathlib, urllib.request\n",
    "# from torchvision.models import ResNet50_Weights\n",
    "\n",
    "# url  = ResNet50_Weights.DEFAULT.url     # gives the same V2 link :contentReference[oaicite:2]{index=2}\n",
    "# path = pathlib.Path(\"checkpoints/resnet50_v2.pth\")\n",
    "# path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# urllib.request.urlretrieve(url, path)   # 97 MB download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cedbf8",
   "metadata": {},
   "source": [
    "### 5.2 Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1cb9a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet50\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "                             \n",
    "# model = resnet50(weights=None)              \n",
    "# model.fc = nn.Linear(model.fc.in_features, NUM_OF_LABELS)  # new head\n",
    "# model.load_state_dict(torch.load(\"checkpoints/resnet50_v2.pth\"), strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7b9ed2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.hub\n",
    "import torch, torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.hub.set_dir(\"checkpoints\") # Sets pytorch to load from checkpoints directory\n",
    "\n",
    "state_path = \"checkpoints/resnet50_v2.pth\"      # wherever you saved it\n",
    "model = resnet50(weights=None) # build a vanilla ResNet-50, no automatic weights\n",
    "\n",
    "# bring in the state_dict\n",
    "state = torch.load(state_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state)                    # all 1000-class params\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# ───── training (add lightweight augmentation if you want) ─────\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),   # optional\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# ───── validation / test ─────\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a3f8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset import MultiLabelDataset\n",
    "\n",
    "\n",
    "train_ds = MultiLabelDataset(train_samples, transform=train_tfms, cache=False)\n",
    "# val_ds   = MultiLabelDataset(val_samples,   transform=val_tfms,   cache=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=32, shuffle=True,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     val_ds, batch_size=32, shuffle=False,\n",
    "#     num_workers=4, pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "00d1b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ImageRecord' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     46\u001b[0m running \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     48\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(imgs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/site-packages/torch/utils/data/dataloader.py:493\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/site-packages/torch/utils/data/dataloader.py:424\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1171\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1164\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1171\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/A2_COMP4329/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ── hyper-params ─────────────────────────────────────────────\n",
    "EPOCH_WARM   = 5                 # head-only\n",
    "EPOCH_FINE   = 20                # full network\n",
    "LR_HEAD      = 1e-3              # warm-up lr\n",
    "LR_BACKBONE  = 1e-4              # fine-tune lr\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BEST_PATH    = Path(\"best_model.pth\")\n",
    "\n",
    "# ── loss ─────────────────────────────────────────────────────\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ── split params into head vs backbone ───────────────────────\n",
    "head_params     = list(model.fc.parameters())\n",
    "backbone_params = [p for n, p in model.named_parameters() if \"fc\" not in n]\n",
    "\n",
    "# freeze backbone for warm-up\n",
    "for p in backbone_params:\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": head_params,     \"lr\": LR_HEAD},\n",
    "        {\"params\": backbone_params, \"lr\": 0.0},       # frozen\n",
    "    ],\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# cosine schedule for the fine-tune phase\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=EPOCH_FINE, eta_min=1e-6\n",
    ")\n",
    "\n",
    "# ── main loop ────────────────────────────────────────────────\n",
    "best_micro = 0.0\n",
    "total_epochs = EPOCH_WARM + EPOCH_FINE\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    # ---------- Train ----------\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item() * imgs.size(0)\n",
    "\n",
    "    # ---------- Un-freeze backbone after warm-up ----------\n",
    "    if epoch + 1 == EPOCH_WARM:\n",
    "        print(\"🔓  Unfreezing backbone …\")\n",
    "        for p in backbone_params:\n",
    "            p.requires_grad = True\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = LR_BACKBONE      # set lr for all groups\n",
    "\n",
    "    # ---------- Validate ----------\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            logits = model(imgs.to(device))\n",
    "            probs  = torch.sigmoid(logits).cpu()          # [B,19]\n",
    "            preds.append((probs >= 0.5).float())\n",
    "            trues.append(labels)\n",
    "    preds = torch.cat(preds)\n",
    "    trues = torch.cat(trues)\n",
    "\n",
    "    micro_f1 = f1_score(trues, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(trues, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02}/{total_epochs} | \"\n",
    "        f\"Train loss {running/len(train_loader.dataset):.4f} | \"\n",
    "        f\"micro-F1 {micro_f1:.3f} | macro-F1 {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---------- Save best ----------\n",
    "    if micro_f1 > best_micro:\n",
    "        best_micro = micro_f1\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(f\"  ✅  New best saved to {BEST_PATH}\")\n",
    "\n",
    "    # LR scheduler (only after un-freezing)\n",
    "    if epoch + 1 > EPOCH_WARM:\n",
    "        scheduler.step()\n",
    "\n",
    "print(f\"\\n🏁  Training finished. Best micro-F1 = {best_micro:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a445333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A2_COMP4329",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
